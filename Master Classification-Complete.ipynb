{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "from pandas import set_option\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#''''''''''''''''''''''''''''''''''''''''''\n",
    "import h2o\n",
    "import time\n",
    "import os\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.grid import H2OGridSearch\n",
    "from h2o.automl import H2OAutoML\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "h2o.init()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset=pd.read_csv('labor1.csv')\n",
    "#dataset=pd.read_csv('labor1.csv',na_values='?')\n",
    "#dataset = dataset.replace('?', numpy.NaN)\n",
    "#dataset=pd.read_csv('sonar.csv',header=None,skiprows=1)\n",
    "#--------------------------------------------------------\n",
    "#dataset=pd.read_excel('labor1.xlsx')\n",
    "#dataset=pd.read_excel('labor1.xlsx',na_values='?')\n",
    "#dataset = dataset.replace('?', numpy.NaN)\n",
    "#dataset=pd.read_excel('sonar.xlsx',header=None,skiprows=1)\n",
    "#--------------------------------------------------------\n",
    "#names=['age','menopause','tumor-size','inv-nodes','node-caps','deg-malig','breast','breast-quad','irradiat','Class']\n",
    "#dataset=pd.read_csv('breast-cancer.csv',header=None,names=names,delim_whitespace=False)\n",
    "#--------------------------------------------------------\n",
    "#dataset=pd.read_csv('segment-challenge.csv',header=None,skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types\n",
    "set_option('display.max_rows', 500)\n",
    "print(dataset.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptions, change precision to 3 places\n",
    "set_option('precision', 3)\n",
    "print(dataset.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptions, change precision to 3 places\n",
    "set_option('precision', 3)\n",
    "print(dataset.describe(include=\"object\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class distribution\n",
    "print(dataset.groupby(60).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEANING/MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING OF MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total missing values of each columns\")\n",
    "print(dataset.isnull().sum())\n",
    "print(\" \")\n",
    "print(\"Total missing values is:\")\n",
    "print(dataset.isnull().any().sum())\n",
    "print(\" \")\n",
    "print(\"missing values(True or False)\")\n",
    "print(dataset.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVAL OF MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = dataset.isnull().sum().sort_values(ascending=False)\n",
    "percent=(dataset.isnull().sum()*100/len(dataset)).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1,keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the number of unique values in each column\n",
    "print(dataset.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete columns with a single unique value\n",
    "# get number of unique values for each column\n",
    "counts = dataset.nunique()\n",
    "# record columns to delete\n",
    "to_del = [i for i,v in enumerate(counts) if v == 1]\n",
    "print(to_del)\n",
    "# drop useless columns\n",
    "dataset.drop(to_del, axis=1, inplace=True)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate rows of duplicate data\n",
    "\n",
    "# calculate duplicates\n",
    "dups = dataset.duplicated()\n",
    "# report if there are any duplicates\n",
    "print(dups.any())\n",
    "# list all duplicate rows\n",
    "print(dataset[dups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete rows of duplicate data from the dataset\n",
    "\n",
    "print(\"actual dataset\",dataset.shape)\n",
    "# delete duplicate rows\n",
    "dataset.drop_duplicates(inplace=True)\n",
    "print(\"new dataset\",dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting the \"Unnamed: 32\" column\n",
    "dataset.drop(\"Unnamed: 32\",axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of column wih higher percentage(missing value)\n",
    "pct_null=dataset.isnull().sum()/len(dataset)\n",
    "missing_features=pct_null[pct_null > 0.70].index\n",
    "dataset.drop(missing_features,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVAL OF COLUMN WITH HIGHER MISSING VALUES\n",
    "dataset=dataset.drop(\"columns\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of missing value from any rows\n",
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR NUMERIC\n",
    "dataset.fillna(dataset.mean(), inplace=True)\n",
    "dataset.fillna(dataset.median(), inplace=True)\n",
    "dataset.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR CATEGORIES ONLY\n",
    "dataset.fillna(dataset.mode(), inplace=True)\n",
    "#FOR CATEGORIES ONLY\n",
    "dataset.fillna((\"No\"), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR NUMERIC AND CATEGORIES\n",
    "dataset=dataset.fillna(method='bfill',limit=2)\n",
    "dataset=dataset.fillna(method='ffill')\n",
    "dataset.interpolate(mmethod='pad',limit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,5))\n",
    "sns.countplot(x=\"job\",hue=\"y\",data=dataset)\n",
    "print(pd.crosstab(dataset[\"job\"],dataset[\"y\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid=sns.FacetGrid(dataset,row=\"Gender\",col=\"Married\",size=2.2,aspect=1.6)\n",
    "grid.map(plt.hist,\"ApplicantIncome\",alpha=0.5,bins=10)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAPPING\n",
    "dataset.customer_type=dataset.customer_type.map({1:'one',2:'two'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INDIVIDUAL ENCODING\n",
    "#conversion of categories value to numeric value\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label=LabelEncoder()\n",
    "dataset[\"class\"]=label.fit_transform(dataset[\"class\"])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding for * categories to numeric value\n",
    "dataset=dataset.select_dtypes(include=['object']).copy()\n",
    "\n",
    "#-------------------------------------------------\n",
    "\n",
    "#conversion of * categories value to numeric value\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label=LabelEncoder()\n",
    "dataset=dataset.apply(label.fit_transform)\n",
    "\n",
    "#---------------------------------------------\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding for both numeric and categories(altogether,conversion of categories value to numeric value\n",
    "\n",
    "dataset_obj=dataset.select_dtypes(include=['object']).copy()\n",
    "dataset_int=dataset.select_dtypes(include=['int64']).copy()\n",
    "dataset_obj.head()\n",
    "dataset_int.head()\n",
    "\n",
    "#-------------------------------------------------\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label=LabelEncoder()\n",
    "dataset_obj=dataset_obj.apply(label.fit_transform)\n",
    "dataset_obj.head()\n",
    "\n",
    "#----------------------------------------------\n",
    "\n",
    "#how to join seperated column\n",
    "\n",
    "dataset=dataset_int.join(dataset_obj)\n",
    "\n",
    "#----------------------------------------------\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding for both numeric and categories(altogether,conversion of categories value to numeric value\n",
    "dataset_float=dataset.select_dtypes(include=['float64']).copy()\n",
    "dataset_obj=dataset.select_dtypes(include=['object']).copy()\n",
    "dataset_int=dataset.select_dtypes(include=['int64']).copy()\n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label=LabelEncoder()\n",
    "dataset_obj=dataset_obj.apply(label.fit_transform)\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "\n",
    "#how to join seperated column\n",
    "\n",
    "dataset1=dataset_int.join(dataset_float)\n",
    "dataset=dataset1.join(dataset_obj)\n",
    "#----------------------------------------------\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation for numeric\n",
    "plt.figure(figsize=(16,5))\n",
    "sns.heatmap(dataset.corr(),annot=True)\n",
    "plt.title(\"Correlation Matrix(for dataset)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTLIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "z=np.abs(stats.zscore(dataset))\n",
    "dataset=dataset[(z<3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversion of types\n",
    "dataset.customer_type=dataset.customer_type.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation for numeric\n",
    "plt.figure(figsize=(16,5))\n",
    "sns.heatmap(dataset.corr(),annot=True)\n",
    "plt.title(\"Correlation Matrix(for dataset)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conversion of types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversion of types\n",
    "dataset.customer_type=dataset.customer_type.astype('int64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.customer_type=dataset.customer_type.map({1:'one',2:'two'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Dataset For Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,0:13] #independent columns\n",
    "Y = dataset.iloc[:,-1] #target column \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURES SELECTION FOR BINARY CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Selection (Chi-squared for classification) 2\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=8)\n",
    "fit = bestfeatures.fit(X,Y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score'] #naming the dataframe columns\n",
    "print(featureScores.nlargest(10,'Score')) #print 10 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#. Feature Importance for classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,Y)\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_,index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVERSAMPLING/UNDERSAMPLING USING SMOTE/NEARMISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=dataset.sample(frac=0.75,random_state=7)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "unseen=dataset.drop(data.index)\n",
    "unseen.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen.to_csv(\"unseen.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape,unseen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,0:19] #independent columns\n",
    "Y = data.iloc[:,-1] #target column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm=SMOTE(random_state=7,k_neighbors=7)\n",
    "X_smote,Y_smote=sm.fit_sample(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smote.shape,Y_smote.shape,X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data=pd.DataFrame(X_smote)\n",
    "Y_data=pd.DataFrame(Y_smote)\n",
    "data_csv=X_data.join(Y_data)\n",
    "data_csv.to_csv(\"data1_csv.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"resampling before smote{}\". format(Counter(Y)))\n",
    "print(\"resampling after smote{}\". format(Counter(Y_smote)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# note:skip validation dataset if using unbalanced classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_h20=h2o.H2OFrame(dataset)\n",
    "#data=h2o.import_file(\"pima.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    data[col].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "corr = data.cor().as_data_frame()\n",
    "corr.index = data.columns\n",
    "sns.heatmap(corr, annot = True, cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation Heatmap\", fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = data.split_frame(ratios=[0.7], seed=1234)\n",
    "response = \"class\"\n",
    "train[response] = train[response].asfactor()\n",
    "valid[response] = valid[response].asfactor()\n",
    "\n",
    "print(\"Number of rows in train, valid and test set : \", train.shape[0], valid.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = data.columns[:-1]\n",
    "gbm = H2OGradientBoostingEstimator()\n",
    "gbm.train(x=predictors, y=response, training_frame=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = gbm.model_performance(valid)\n",
    "print(perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopping_metric = \"AUC\",\n",
    "gbm_tune = H2OGradientBoostingEstimator(\n",
    "    ntrees = 1000,\n",
    "    learn_rate = 0.01,\n",
    "    stopping_rounds = 20,\n",
    "    stopping_metric = \"AUC\",\n",
    "    col_sample_rate = 0.7,\n",
    "    sample_rate = 0.7,\n",
    "    seed = 1234\n",
    ")      \n",
    "gbm_tune.train(x=predictors, y=response, training_frame=train, validation_frame=valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_tune.model_performance(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopping_metric = \"AUC\",\n",
    "from h2o.grid.grid_search import H2OGridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_grid = H2OGradientBoostingEstimator(\n",
    "    ntrees = 1000,\n",
    "    learn_rate = 0.01,\n",
    "    stopping_rounds = 20,\n",
    "    stopping_metric = \"AUC\",\n",
    "    col_sample_rate = 0.7,\n",
    "    sample_rate = 0.7,\n",
    "    seed = 1234\n",
    ") \n",
    "\n",
    "hyper_params = {'max_depth':[4,6,8,10,12]}\n",
    "grid = H2OGridSearch(gbm_grid, hyper_params,\n",
    "                         grid_id='depth_grid')\n",
    "#Train grid search\n",
    "grid.train(x=predictors, \n",
    "           y=response,\n",
    "           training_frame=train,\n",
    "           validation_frame=valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_grid = grid.get_grid(sort_by='auc',decreasing=True)\n",
    "print(sorted_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.model_performance(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopping_metric = \"AUC\",\n",
    "cv_gbm = H2OGradientBoostingEstimator(\n",
    "    ntrees = 3000,\n",
    "    learn_rate = 0.05,\n",
    "    stopping_rounds = 20,\n",
    "    stopping_metric = \"AUC\",\n",
    "    nfolds=4, \n",
    "    seed=2018)\n",
    "cv_gbm.train(x = predictors, y = response, training_frame = train, validation_frame=valid)\n",
    "cv_summary = cv_gbm.cross_validation_metrics_summary().as_data_frame()\n",
    "cv_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_gbm.model_performance(valid).auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H20 with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators import H2OXGBoostEstimator\n",
    "\n",
    "cv_xgb = H2OXGBoostEstimator(\n",
    "    ntrees = 1000,\n",
    "    learn_rate = 0.05,\n",
    "    stopping_rounds = 20,\n",
    "    stopping_metric = \"AUC\",\n",
    "    nfolds=4, \n",
    "    seed=2018)\n",
    "cv_xgb.train(x = predictors, y = response, training_frame = train, validation_frame=valid)\n",
    "cv_xgb.model_performance(valid).auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_xgb.varimp_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML : Automatic Machine Learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "aml = H2OAutoML(max_models = 10, max_runtime_secs=60, seed = 1)\n",
    "aml.train(x=predictors, y=response, training_frame=train, validation_frame=valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = aml.leaderboard\n",
    "lb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pred=aml.leader.predict(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml.predict(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml.leader.model_performance(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids=list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALIDATION DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-out validation dataset\n",
    "array = dataset.values\n",
    "X = array[:,0:60].astype(float)\n",
    "Y = array[:,60]\n",
    "validation_size = 0.20\n",
    "seed = 7\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y,\n",
    "test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALGORITHM CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',\n",
    "LogisticRegression())])))\n",
    "pipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()),('LDA',\n",
    "LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN',\n",
    "KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART',\n",
    "DecisionTreeClassifier())])))\n",
    "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB',\n",
    "GaussianNB())])))\n",
    "pipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC())])))\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Scaled Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune scaled SVM\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n",
    "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "param_grid = dict(C=c_values, kernel=kernel_values)\n",
    "model = SVC()\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune scaled KNN\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "neighbors = [1,3,5,7,9,11,13,15,17,19,21]\n",
    "leaf_size=[1,2,3,5]\n",
    "weights=['uniform', 'distance']\n",
    "param_grid = dict(n_neighbors=neighbors,leaf_size=leaf_size,weights=weights)\n",
    "model = KNeighborsClassifier()\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune scaled Logistic\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n",
    "solver_values = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "param_grid = dict(C=c_values, solver=solver_values)\n",
    "model = LogisticRegression()\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune scaled LinearDiscriminantAnalysis\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "#c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n",
    "solver_values = ['svd', 'lsqr', 'eigen']\n",
    "param_grid = dict(solver=solver_values)\n",
    "model = LinearDiscriminantAnalysis()\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune scaled DecisionTreeClassifier\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "max_features= ['auto', 'sqrt', 'log2']\n",
    "min_samples_split = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "min_samples_leaf= [1,2,3,4,5,6,7,8,9,10,11]\n",
    "param_grid = dict(max_features=max_features,min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "model = DecisionTreeClassifier(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune scaled GaussianNB\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "#var_smoothing_values = ['1e-09']\n",
    "#priors_values = ['None']\n",
    "param_grid = dict()\n",
    "model = GaussianNB()\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSEMBLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensembles1\n",
    "ensembles = []\n",
    "ensembles.append(('AB', AdaBoostClassifier()))\n",
    "ensembles.append(('GBM', GradientBoostingClassifier()))\n",
    "ensembles.append(('RF', RandomForestClassifier()))\n",
    "ensembles.append(('ET', ExtraTreesClassifier()))\n",
    "results = []\n",
    "names = []\n",
    "for name, model in ensembles:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Ensemble Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune scaled AdaBoostClassifier()\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "n_estimators=[50,100,150]\n",
    "learning_rate=[0.1,0.01,0.001,1.0]\n",
    "param_grid = dict(n_estimators=n_estimators,learning_rate=learning_rate)\n",
    "model =AdaBoostClassifier(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune scaled GBM\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "param_grid = dict(n_estimators=numpy.array([50,100,150,200,250,300,350,400]))\n",
    "model = model = GradientBoostingClassifier(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune scaled Randomforest\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "criterion=['gini','entropy']\n",
    "min_samples_split = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "min_samples_leaf= [1,2,3,4,5,6,7,8,9,10,11]\n",
    "n_estimators=[10,15,20,25,30]\n",
    "param_grid = dict(criterion=criterion,min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,n_estimators=n_estimators)\n",
    "model = RandomForestClassifier(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune scaled ExtraTreesClassifier\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "\n",
    "min_samples_split = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "min_samples_leaf= [1,2,3,4,5,6,7,8,9,10,11]\n",
    "\n",
    "param_grid = dict(min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "model = ExtraTreesClassifier(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensembles2\n",
    "ensembles = []\n",
    "ensembles.append(('LGBMC', LGBMClassifier()))\n",
    "ensembles.append(('XGBC', XGBClassifier()))\n",
    "ensembles.append(('Cat', CatBoostClassifier()))\n",
    "results = []\n",
    "names = []\n",
    "for name, model in ensembles:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune scaled LGBMClassifier\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "n_estimators=[50,100,150]\n",
    "boosting_type=['gbdt','dart','goss','rf']\n",
    "param_grid = dict(n_estimators=n_estimators,boosting_type=boosting_type)\n",
    "model = LGBMClassifier(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune scaled XGBClassifier\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "learning_rate=[0.1,0.01,0.001,0.0001]\n",
    "n_estimators=[100,150,200,250]\n",
    "max_depth=[2,3,4,5,6,7,8,9,10]\n",
    "param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators,max_depth=max_depth)\n",
    "model = XGBClassifier(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune scaled CatBoostClassifier\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "learning_rate=[0.1,0.01,0.001,0.0001]\n",
    "boosting_type=['gbdt','dart','goss','rf']\n",
    "param_grid = dict(learning_rate=learning_rate, boosting_type=boosting_type)\n",
    "model = CatBoostClassifier(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From overampling/undersampling for unseen dataset\n",
    "X_validation = unseen.iloc[:,0:19].values #independent columns\n",
    "Y_validation = unseen.iloc[:,-1].values #target column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "model = SVC(C=1.5)\n",
    "model.fit(rescaledX, Y_train)\n",
    "# estimate accuracy on validation dataset\n",
    "rescaledValidationX = scaler.transform(X_validation)\n",
    "predictions = model.predict(rescaledValidationX)\n",
    "print(accuracy_score(Y_validation, predictions))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions))\n",
    "print(roc_auc_score(Y_validation,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "pickle.dump(model, open('model.pkl','wb'))\n",
    "\n",
    "loaded_model = pickle.load(open('model.pkl','rb'))\n",
    "print(loaded_model.predict([[4, 300, 500]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on 33%\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "#-------------------------\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "dump(model, open(filename, 'wb'))\n",
    "#--------------------------\n",
    "# some time later...\n",
    "# load the model from disk\n",
    "loaded_model = load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR MEDICAL RECORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_validation=Y_validation.astype(int)\n",
    "predictions=predictions.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the truth and predicted values\n",
    "print ('True', Y_validation[0:20])\n",
    "print ('pred', predictions[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting the classification threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(rescaledValidationX)[0:10] #print the first 10 predicited response\n",
    "#------------------------------------\n",
    "model.predict_proba(rescaledValidationX)[0:10,:] #print the first 10 predicted probabilities response for the class memeber\n",
    "#-------------------------------------\n",
    "model.predict_proba(rescaledValidationX)[0:10,1]  #print the first 10 predicted probabilities of one\n",
    "#-------------------------------------\n",
    "y_pred_prob=model.predict_proba(rescaledValidationX)[:,1]  #store the predicted probabilites for class one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot the probabilities values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_pred_prob,bins=8,  facecolor=\"blue\", alpha=0.5)\n",
    "plt.xlim(0,1)\n",
    "plt.xlabel(\"predicted probablities of diabetes\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.title(\"Histogram of predicited probabilities\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decrease the threshold for predicting diabetes in order to increase the sensitivity of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob=model.predict_proba(rescaledValidationX)[:,1]\n",
    "y_pred_prob=y_pred_prob.reshape(1,-1)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict diabetes if the predicted probabilities is greater than 0.3\n",
    "\n",
    "from sklearn.preprocessing import binarize\n",
    "y_pred_class=binarize(y_pred_prob,0.30)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare the final threshold model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "model = LogisticRegression(C=0.1, solver='newton-cg')\n",
    "model.fit(rescaledX, Y_train)\n",
    "# estimate accuracy on validation dataset\n",
    "rescaledValidationX = scaler.transform(X_validation)\n",
    "predictions = model.predict(rescaledValidationX)\n",
    "print(accuracy_score(Y_validation, y_pred_class))\n",
    "print(confusion_matrix(Y_validation, y_pred_class))\n",
    "print(classification_report(Y_validation,y_pred_class))\n",
    "print(roc_auc_score(Y_validation,y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert and print the resulted predicted class/values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_validation=Y_validation.astype(int)\n",
    "predictions=predictions.astype(int)\n",
    "y_pred_class=y_pred_class.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ( Y_validation[0:20],'true value')\n",
    "print ( predictions[0:20],'predited value')\n",
    "print(y_pred_class[0:20],'value after using threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print the confusion matrix for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((\"\"))\n",
    "print((\"confusion matrix without using threshold 0.25\"))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print((\"\"))\n",
    "print((\"confusion matrix  using threshold 0.25\"))\n",
    "print(confusion_matrix(Y_validation, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
